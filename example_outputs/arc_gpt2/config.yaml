paths:
  root_dir: .
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
notes: Demonstration usage for Laplace-LoRA method.
task_name: ${dset.name}_${llm.name}
log_level: INFO
seed: null
print_config: false
opt:
  module: torch.optim
  classname: AdamW
  lr: 5.0e-05
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-05
  weight_decay: 0.1
dset:
  name: arc
  max_epochs: 5
  train_bs: 6
  train_split: train
  eval_bs: 4
  eval_split: validation
  eval_subset: 1500
  max_length: 256
  train_subset: -1
llm:
  quantization:
    _target_: transformers.utils.quantization_config.BitsAndBytesConfig
    load_in_4bit: true
    load_in_8bit: false
    llm_int8_threshold: 6.0
    llm_int8_has_fp16_weight: false
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true
  peft:
    _target_: peft.LoraConfig
    r: 8
    lora_alpha: 8
    lora_dropout: 0.05
    task_type: CAUSAL_LM
    inference_mode: false
    bias: none
    target_modules:
    - c_attn
    - c_proj
    - c_fc
    - lm_head
  name: gpt2
  model_name_or_path: gpt2
  config_class: AutoConfig
  config_kwargs: {}
  tokenizer_class: AutoTokenizer
  tokenizer_kwargs:
    use_fast: true
    padding_side: left
  tokenizer_special_tokens:
    pad_token: tokenizer.bos_token
  model_class: AutoModelForCausalLM
  model_kwargs:
    torch_dtype: auto
    low_cpu_mem_usage: true
  global_gen_kwargs: {}
  add_space: true
  is_s2s: false
  use_peft: true
  use_quant: true
run_every_step: true
use_tqdm: true
map_lr: 5.0e-05
train_steps: 1000
n_kfac: 10
lr_threshold: 100
prior_var: 0.1
tokenizer_run_kwargs:
  padding: true
  truncation: true
  return_tensors: pt
  max_length: 512
out_dir: null
